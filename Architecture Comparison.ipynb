{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, f1_score\n",
    "\n",
    "from source import read_preprocessed_data\n",
    "from transform import load_process_and_store_spreadsheets\n",
    "from pipelines import benchmark_pipelines, build_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_process_and_store_spreadsheets()\n",
    "\n",
    "X_train, y_train = read_preprocessed_data(\"output/train.tsv\")\n",
    "X_test, y_test = read_preprocessed_data(\"output/test.tsv\")\n",
    "ps = build_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Richard_train</th>\n",
       "      <th>Richard_test</th>\n",
       "      <th>Julian_train</th>\n",
       "      <th>Julian_test</th>\n",
       "      <th>Lev_train</th>\n",
       "      <th>Lev_test</th>\n",
       "      <th>Freeman_train</th>\n",
       "      <th>Freeman_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.552632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.770115</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.605263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.763158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.763158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Richard_train  Richard_test  Julian_train  \\\n",
       "DecisionTreeClassifier           0.862069      0.631579      0.632184   \n",
       "RandomForestClassifier           0.954023      0.736842      0.770115   \n",
       "GradientBoostingClassifier       0.816092      0.710526      0.781609   \n",
       "KNeighborsClassifier             1.000000      0.763158      0.896552   \n",
       "DummyClassifier                  0.724138      0.763158      0.724138   \n",
       "\n",
       "                            Julian_test  Lev_train  Lev_test  Freeman_train  \\\n",
       "DecisionTreeClassifier         0.394737   0.896552  0.710526       0.816092   \n",
       "RandomForestClassifier         0.605263   0.965517  0.710526       0.839080   \n",
       "GradientBoostingClassifier     0.789474   0.781609  0.763158       0.827586   \n",
       "KNeighborsClassifier           0.657895   1.000000  0.815789       1.000000   \n",
       "DummyClassifier                0.763158   0.724138  0.763158       0.724138   \n",
       "\n",
       "                            Freeman_test  \n",
       "DecisionTreeClassifier          0.552632  \n",
       "RandomForestClassifier          0.605263  \n",
       "GradientBoostingClassifier      0.763158  \n",
       "KNeighborsClassifier            0.526316  \n",
       "DummyClassifier                 0.763158  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_pipelines(\n",
    "    ps, X_train, y_train[\"response_grouped\"], X_test, y_test[\"response_grouped\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1243: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if pos_label not in present_labels:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pos_label=1 is not a valid label: array(['non responder (sd+pd)', 'responder (pr+cr)'], dtype='<U21')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a362d14d0548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m benchmark_pipelines(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response_grouped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response_grouped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m~/avenio/pipelines.py\u001b[0m in \u001b[0;36mbenchmark_pipelines\u001b[0;34m(pipelines, X_train, y_train, X_test, y_test, metric)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             benchmark_result[classifier_name][f\"{pipeline_name}_train\"] = metric(\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             )\n\u001b[1;32m    181\u001b[0m             benchmark_result[classifier_name][f\"{pipeline_name}_test\"] = metric(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m   1058\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1180\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1183\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1415\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m                     raise ValueError(\"pos_label=%r is not a valid label: \"\n\u001b[0;32m-> 1246\u001b[0;31m                                      \"%r\" % (pos_label, present_labels))\n\u001b[0m\u001b[1;32m   1247\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pos_label=1 is not a valid label: array(['non responder (sd+pd)', 'responder (pr+cr)'], dtype='<U21')"
     ]
    }
   ],
   "source": [
    "benchmark_pipelines(\n",
    "    ps, X_train, y_train[\"response_grouped\"], X_test, y_test[\"response_grouped\"], metric=f1_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
